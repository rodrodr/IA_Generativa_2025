---
title: "<b>Investigación </b><br>Empleando la IA"
---

```{r setup, echo=F, include=F}

eval <- TRUE

library(rollama)
library(ellmer)
library(tenet)
library(stringi)
library(quanteda)
library(reactable)

# Calcula la similitud entre vectores
cosSim <- function(a,b, normalize=TRUE){
  # assuming unit vectors
  # the cosine is just the dot-product
  
  if(normalize==FALSE){
    sim <- a %*% b
  }else{
    
    a_nrm <- sqrt(rowSums(a^2))
    
    b_nrm <- sqrt(sum(b^2))
    
    a_nrm <- a / a_nrm
    b_nrm <- b / b_nrm
    
    cs <- rowSums(a_nrm %*% b_nrm)
    
    nrm_sim <- (cs + 1) / 2
    
    
    sim <- nrm_sim
  }
  
  return(sim)
}

# Selecciona los textos más silimares, sea por
# el número de más similares o por el valor de
# la similitud
simTop <- function(sim, top_n=5, top_val=NULL){
  
  rx <- data.frame(id=1:length(sim), sim=sim)  
  
  rx <- rx[order(rx$sim,decreasing=TRUE),]
  
  if(is.null(top_val)==TRUE){
    rx <- dplyr::top_n(rx, top_n)
  }else{
    rx <- rx[rx$sim > top_val,]  
  }
  
  return(rx$id)  
  
}


```


<style>
 .side{
    display: inline-block; 
    float: left;
}

  span{
    position: relative;
    display: inline-flow;
    cursor: pointer;
    padding: 3px;
}


  span .tooltiptext {
    visibility: hidden;
    background-color: inherit;
    border-radius: .25em;
    color: #fff;
    text-align: center;
    padding: 3px;
    margin-top: 10px;
    position: absolute;
    z-index: 1;
    top: -90%;
    left: 40%;
    transform: translateY(-50%);
    opacity: 0;
    transition: opacity 0.5s;
}

  .span:hover .tooltiptext {
    visibility: visible;
    opacity: 1;
}

  .pt_persona{
    background: #e41a1c;
    color: white;
}

  .pt_tarea{
    background: #377eb8;
    color: white;
}

  .pt_contexto{
    background: #4daf4a;
    color: white;
}

  .pt_ejemplo{
    background: #a65628;
    color: white;
}

  .pt_tono{
    background: #f781bf;
    color: white;
}

  .pt_salida{
    background: #984ea3;
    color: white;
}

  .pt_restrict{
    background: #ff7f00;
    color: white;
}

  .prompt{
    background: #fcf3cf;
    padding: 10px;
}
  

</style>


> Una IA estresada entra a un bar y le dice al camarero:
> 
> — ¡Ponme un byte doble, que hoy tuve que responder 10.000 correos, predecir el clima en Marte y enseñarle a un gato a programar en Python!
>
> El camarero la mira y responde:
> 
> — ¿Y lo del gato funcionó?

> — ¡¡Sí!! ¡¡Pero ahora me exige sindicalización y días de ron libre!!
>
> **chatGPT**

## La IA generativa en la investigación

El esta sesión vamos a emplear la IA para realizar algunas tareas de investigación. Cosas muy sencillas, pero útiles para que tengamos una idea de cómo esta nueva herramienta puede ser aplicada en nuestro trabajo diario.

## Primeros pasos: Ollama y API

En este apartado vamos a ver cómo usar la API de Ollama y de Gemini en R. Estos dos recursos permitirán que podamos emplear los modelos de IA generativa tanto de forma local como remota. Los modelos locales requieren ordenadores con mayor capacidad de procesamiento, mientras que las API tienen un coste asociado. En este caso, vamos a usar la API de Gemini, que es gratuita hasta un límite de uso.

### Ollama

Para usar Ollama, primero debemos instalarlo. Las instrucciones se encuentran en la página inicial del curso. Además, tenéis que tener el R y el RStudio también instalados en vuestro ordenador. También en la página inicial podéis encontrar una lista de los paquetes que debéis instalar dentro de R para que el código funcione.

El código abajo ejecuta un *prompt* sencillo en Ollama:

```{r ollama_ejemplo, eval=F}

# Carga el paquete rollama
# (certifícate de que ollama 
# está siendo ejecutado en tu 
# ordenador antes de todo)
library(rollama)

# Define el modelo que quieres emplear 
model <- "gemma2:2b"

# Realiza la pregunta
query("¿Cuál es la capital de España?",
      model=model)

```

<br>

A partir de ahora, todo lo que viene a continuación sería complejizar el *prompt* para que realice una tarea de investigación más compleja. 

Por otra parte, si queremos emplear una API, lo primero es obtener una *token*, una especie de clave única que identifica a cada usuario. La clave de API de Google se puede obtener en [Google AI Studio](https://aistudio.google.com/u/0/apikey){target="_blank"}. Una vez obtenida la clave, podemos emplear el código abajo para realizar la misma consulta que antes, pero ahora a través de la API de Gemini:

```{r api_ejemplo, eval=F}

library(ellmer)

# Define el modelo que quieres emplear
model <- "gemini-2.0-flash-lite"

# Copia y pega la API key que obtuviste en la 
# página de Google aquí
key <- "AAAAAAAAAAAaAsdosdsooasjoaaazcvoeprjwe"

# Crea un chat para interactuar con el modelo
chat <- chat_gemini(
          base_url = "https://generativelanguage.googleapis.com/v1beta/",
    model = model,
    api_key = key)

# Pregúntale lo que sea
chat$chat("¿Cuál es la capital de España?")

```

## Clasificación de textos

Una aplicación adicional de la IA generativa consiste en la clasificación de textos. En este caso, el modelo puede analizar un conjunto de documentos y clasificarlos en diferentes categorías o etiquetas según su contenido. Esto es especialmente útil en tareas como la organización de grandes volúmenes de información, la detección de spam o la categorización de correos electrónicos. En la ciencia política, las aplicaciones comunes incluyen el análisis de contenido ideológico, sentimientos, temas y otros aspectos relacionados con la opinión pública. 

En este apartado del curso, examinaremos cómo podemos emplear la IA generativa para la tarea de clasificación de textos. Para efectos didácticos, dividimos el proceso en cuatro pasos: 1) la selección de textos y modelo; 2) la creación del *prompt* y los ajustes del modelo; 3) la introducción de ejemplos; y, finalmente, 4) la revisión de los resultados.


### Paso 1. Selección de los textos y del modelo

El primer paso consiste en cargar los paquetes necesarios para la tarea, definir los textos que queremos clasificar y elegir un modelo que pueda llevar a cabo el trabajo. En este caso, vamos a emplear el paquete `rollama` para cargar un modelo pequeño, el `gemma2:2b` de 2.6 mil millones de parámetros (2.6B). Podríamos hacer lo mismo empleando la API de Google, por ejemplo. En este último caso, tendríamos que emplear el paquete `ellmer` y el modelo `gemini-flash-2.0-lite`, por ejemplo. Elegimos la primera opción porque se trata de un modelo pequeño que puede funcionar en la mayoría de los ordenadores personales. Sin embargo, como veremos, no nos suministrará siempre los mejores resultados.

```{r clasif_text}

# carga los paquetes
library(rollama)
library(tibble)
library(purrr)

# Introduce los textos que queremos clasificar
tx <- c("La evidencia respalda esta teoría.",
  "Es importante realizar más experimentos para confirmar los resultados de nuestra investigación.",
  "¿Qué implicaciones tienen estos hallazgos para el futuro?",
  "El avance tecnológico está transformando nuestras vidas.",
  "Necesitamos actualizar el sistema operativo para que funcione mejor.",
  "¿Ya probaste reiniciar el dispositivo?",
  "Es crucial participar en las elecciones para hacer valer nuestra voz.",
  "Las decisiones políticas afectan directamente nuestra vida diaria.",
  "El debate se centró en temas económicos y sociales.",
  "El trabajo en equipo es clave para ganar.",
  "Ese fue un gol increíble, ¡cambió todo el partido!",
  "La preparación física y mental es esencial para los atletas.",
  "El arte refleja aspectos profundos de nuestra sociedad.",
  "El concierto de esta noche será inolvidable.")

# Elegimos el modelo que nos gustaria 
# emplear (aquí emplearemos inicialmente
# el gemma2 2b, que es el mas rapido y
# que funcionara en la mayoria de los
# ordenadores)
modelo <- "gemma2:2b"

```

### Paso 2. Definición del *prompt* y ajuste del modelo

Acto seguido ya podemos pedir al modelo que clasifique los textos que hemos seleccionado. Para ello debemos definir de forma clara el `prompt` que emplearemos para solicitar la clasificación. Primero, definimos su `persona`. En este caso, será un sistema especializado en clasificación de datos. Luego, establecemos la `tarea`, que será asignar una categoría única cada uno de los textos. Tercero, imponemos un `formato` de salida, que debe ser el texto de una de las categorías definidas. Finalmente, suministramos el `texto`a ser clasificado y las `categorías` posibles. 
 
Una vez definido el `prompt`, configuramos el modelo para que nos devuelta los resultados como texto. Además, configuramos dos parámetros muy importantes: el `seed`, o la semilla estadística, que garantizará resultados consistentes, y la `temperatura`, que controla el grado de "tolerancia" del modelo a respuestas menos frecuentes. Cuanto menor la temperatura, más descriptivo el modelo (y menos propenso a alucinaciones o errores de clasificación).  

Ejecutamos el código abajo y estos son los resultados:

```{r clasif_loop}

# Crea un data.frame vacio
# para acumular los resultados
d <- data.frame()

# Para cada texto
for(i in 1:length(tx)){

  # Definimos nuestro prompt de forma
  # sistematica para que nos devuelva 
  # solo la categoría deseada
  q <- tribble(
    ~role,    ~content,
    "system", "Eres un sistema especializado en clasificación de datos. Tienes que clasificar textos en una única categoría. Responda solamente con la categoría correcta.",
    "user",   paste0("texto: ", tx[i],"\ncategorias: ciencia, tecnología, política, deportes, entretenimiento, cultura"))
  
  # Llamamos al modelo para que clasifique
  out <- query(q, 
               model = modelo, 
               screen = FALSE, 
               output = "text",             
               model_params = list(
                 seed=123,
                 temperature=0))

  # Acumulamos los resultados
  d <- rbind(d, 
             data.frame(texto=tx[i], 
                        categoria=out))

}

# Mostramos los resultados
reactable(d, 
          sortable = T, 
          resizable = T)

```

En lugar de un bucle enfarragoso, podemos utilizar una estrategia más elegante y vectorizar el proceso. Además de ser más limpio, resulta incluso más rápido procesar las informaciones.

```{r clasif_batch}

# Preprocesamos las consultas con anterioridad
q <- make_query(
  template = "{text}\n{prompt}",
  text = tx,
  prompt = "Categorias: ciencia, tecnología, política, deportes, entretenimiento, cultura",
  system = "Eres un sistema especializado en clasificación de datos. Tienes que clasificar textos en una única categoria. Responda solamente con la categoría correcta.")
  
out <- query(q, 
             model = modelo, 
             screen = FALSE, 
             output = "text",
             model_params = list(
               seed=123,
               temperature=0))
  
d <- data.frame(texto=tx, categoria=out)

# Mostramos los resultados
reactable(d, 
          sortable = T, 
          resizable = T)

```


Como vemos, la clasificación de las respuestas resulta bastante buena. Solamente en un caso - el texto "El avance tecnológico está transformando nuestras vidas."- el modelo duda entre ciencia y tecnología. Aun así es un resultado bastante bueno, puesto que se tratan de temas que se acercan mucho. Si empleamos un modelo con más parámetros, como el `gemma3:27b` o el `Qwen3:32b`, los resultados serán aún mejores. No obstante, tales modelos requieren ordenadores más pontentes para procesar las informaciones. Como regla general, sugiero que busquéis el modelo más eficiente compatible con vuestro *hardware*, es decir, el que haga el trabajo mejor y en menos tiempo dentro de las limitaciones del equipo que tengáis. Si vuestro ordenador no soporta la carga de procesamiento de un modelo local de IA, es recomendable emplear una API, como da de Google. De todos modos, como veremos a continuación, existen estrategias que pueden ayudar a mejorar la calidad de las respuestas sin cambiar de modelo o tener que emplear un ordenador más potente.

### Paso 3. Mejora de la clasificación con ejemplos

Una forma de mejorar la calidad de las respuestas es suministrar ejemplos al modelo. Estos ejemplos deben ser textos que representen claramente cada una de las categorías posibles. De esta forma, el modelo podrá aprender de los ejemplos y mejorar su capacidad de clasificación. Se trata de una manera rápida y menos costosa de especializar el modelo en la tarea deseada. Con relativamente pocas observaciones, podemos lograr que precisión aumente.

```{r clasif_ejemplos}

# Crea una base de datos de ejemplos
eje <- tribble(
  ~text, ~answer,
  "El método científico es fundamental para obtener resultados confiables.","ciencia",
  "La investigación interdisciplinaria está cobrando cada vez más importancia.","ciencia",
  "Es vital analizar los datos antes de sacar conclusiones precipitadas.","ciencia",
  "La inteligencia artificial está cambiando la forma en que trabajamos.","tecnología",
  "El software necesita una actualización para solucionar los errores.","tecnología",
  "La ciberseguridad es una prioridad en la era digital.","tecnología",
  "El consenso es esencial para avanzar en las negociaciones.","política",
  "Las políticas públicas deben responder a las necesidades de la ciudadanía.","política",
  "La transparencia en el gobierno genera confianza en la población.","política",
  "La dedicación y la disciplina son esenciales para alcanzar el éxito.","deportes",
  "El entrenador ajustó la estrategia para el segundo tiempo.","deportes",
  "El torneo fue una gran oportunidad para los jugadores jóvenes.","deportes",
  "Los premios de esta noche reconocen lo mejor del cine y la televisión.","entretenimiento",
  "La trama de la película es impredecible y emocionante.","entretenimiento",
  "Ese concierto fue una experiencia inolvidable para los fans.","entretenimiento",
  "Los festivales tradicionales son una muestra de nuestras raíces.","cultura",
  "La globalización está influyendo en las expresiones culturales locales.","cultura",
  "El patrimonio cultural debe preservarse para las futuras generaciones.","cultura")

# Prepara las consultas
q <- make_query(
  template = "{text}\n{prompt}",
  text = tx,
  prompt = "Categorias: ciencia, tecnología, política, deportes, entretenimiento, cultura",
  system = "Eres un sistema especializado en clasificación de datos. Tienes que clasificar textos en una única categoria. Responda solamente con la categoría correcta.",
  examples = eje)

# Lleva a cabo la clasificación
out <- query(q, 
             model = modelo, 
             screen = FALSE, 
             output = "text",
             model_params = list(
               seed=123,
               temperature=0))

# Crea un data.frame con 
# los textos y las categorias
# resultantes
d <- data.frame(texto=tx, 
                categoria=out)

# Mostramos los resultados
reactable(d, 
          sortable = T, 
          resizable = T)

```

Como vemos, la clasificación de los textos ha mejorado notablemente. En todos los casos, el modelo ha acertado la categoría. La razón de esta mejora es que hemos proporcionado ejemplos al modelo. Todas las categorías ahora correctamente representadas. 

### Revisión general

Los modelos LLM pueden ser herramientas útiles para clasificar textos. No obstante, para sacar el mejor provecho posible de sus capacidades, tenemos que seguir una secuencia sencilla de pasos que nos ayudan a aumentar la precisión de los resultados e integrar el proceso en un flujo más amplio de análisis de datos.

Aquí van los pasos esenciales:

**1. Prepara los datos** para que estén listos para ser procesados por el modelo.

**2. Formula un prompt** que sea claro y defina la persona, la tarea, el contexto y el formato de salida de la consulta. Un truco es emplear el mismo modelo para revisar y mejorar el `prompt`. Creáis un primer borrador y luego pedís al modelo que de sugerencias de mejora.

**3. Selecciona el modelo** que mejor se ajuste a tus necesidades y a las capacidades de tu ordenador.

**4. Suministra ejemplos** al modelo con algunos textos ya clasificados. Eso puede aumentar la precisión de las respuestas.

**5. Evalúa los resultados** y ajusta o cambia el `prompt` o modelo si es necesario.


## Búsqueda semántica y RAG

Las búsquedas semánticas son una de las aplicaciones más comunes de la IA generativa. 

En este caso, el modelo no solo busca palabras clave en un texto, sino que también intenta comprender el significado y la intención detrás de la consulta. Esto se logra mediante el uso de técnicas avanzadas de procesamiento del lenguaje natural (NLP) y modelos de aprendizaje profundo.


```{r embeddings, eval=eval}

library(tenet)
library(quanteda)
library(reactable)

cp <- corpus(spa.inaugural)

cp <- corpus_reshape(cp, to = "paragraphs")

d <- quanteda::convert(cp, to = "data.frame")
d <- d[d$text!="",]

d$nwords <- stri_count_words(d$text)
d <- d[d$nwords>10,]

emb <- embed_text(d$text, model = "snowflake-arctic-embed2")

ex <- sapply(emb, as.numeric)

d$text_embed <- ex

q <- "¿Cuál es la posición de los presidentes frente a los temas de seguridad, tanto ciudadana como exterior? ¿Cuál es el rol de las organizaciones internacionales como FRONTEX o la OTAN?"

quest <-embed_text(q, model="snowflake-arctic-embed2")

sim <- cosSim(d$text_embed, as.numeric(quest), normalize=T)

d$similitud <- round(sim,3)

nn <-simTop(sim, top_n=50, top_val=NULL)

dx <- d[nn, c("President","text","similitud")]

reactable(dx, 
          sortable = T, 
          resizable = T)

```


Ahora pasamos los resultados se la búsqueda semántica al modelo


```{r embeddings_ollama, eval=eval, results='asis'}

tt <- paste0("{'Presidente': '", dx$President, "', 'Texto': '", dx$text, "'}", collapse = ",\n")


prompt <- paste0("Actúa como un experto en el análisis de textos. A continuación tienes una serie de textos Por favor, analiza los textos y responde, con base en los mismos, a la pregunta:", q, ". Genera el resultado en formato markdown. Los textos son los siguientes:\n\n'", tt)


out <- query(prompt, model="gemma3:27b", output = "text", model_params = list(
  temperature = 1,
  num_ctx = 32000))

cat(out)

```


## Extracción de datos

Otro uso que podemos dar a los modelos de IA es la extracción de datos de textos. En muchos casos, los datos que necesitamos están en textos que no están estructurados o que resultan difíciles de extraer de forma organizada.  Por ejemplo, en el caso de las actas de reuniones, los informes de asistencia o los resúmenes de eventos, los datos pueden estar dispersos y ser difíciles de analizar. Los modelos LLM pueden ayudarnos a convertir esos datos en tablas o bases de datos que podamos analizar con mayor facilidad.

La figura abajo contiene un listado de asistencia de la sesión plenaria del Congreso de Perú llevada a cabo  el 14 de junio de 2017. En total, el documento contiene 9 páginas de asistencia y otras 9 conteniendo votaciones. Infelizmente, el pdf está escaneado y solo podemos acceder a imágenes. Como tarea previa, he llevado a cabo el reconocimiento de caracteres (OCR). Pero aún así, hace falta convertir los datos en una tabla que podamos analizar.


![](pag_PER.jpg)

Como podemos ver, el acta de asistencia está organizado en tres grandes bloques. El primero es la identificación de la sesión, con la fecha, la hora y el período legislativo. El segundo apartado se divide en tres columnas con el grupo parlamentario, los apellidos y los nombres de los diputados y el tipo de asistencia. El bloque final contiene los datos agregados. Nos interesa extraer los datos del segundo bloque y convertirlos en un `data.frame`. 

El primer paso consiste en cargar los textos de asistencia extraidos del pdf original. Ya he creado un objeto `RData` con los textos que podemos descargar de la nube.

```{r extract_prepara}

library(readr)
library(stringi)
library(readtext)
library(httr)

# Carga los datos de texto almacenados 
# en la nube
response <- GET(url="https://github.com/rodrodr/cieps2025/raw/ccadffc3bbf8f510ae013278336da8a441bf2c0d/asistencias.RData")

# Los guarda en el ordenador
writeBin(response$content, "asistencia.Rdata")

# Carga el archivo
load("asistencia.Rdata")

```


Ahora pasamos a la extracción de los datos de los textos. Como en los ejemplos anteriores, he preparado un par de ejemplos para que el modelo pueda identificar qué quiero que extraiga y en qué formato. De forma más concreta, he pedido que separe los datos de un mismo diputado con un punto y coma y que se separen las columnas con un salto de línea.

El modelo elegido ha sido el `gemma2:2b`, pequeño y rápido, especialmente indicado para grandes volúmenes de texto. Para aumentar la precisión y velocidad, he aislado el segundo bloque con los nombres y, luego, dividi el texto por líenas. De ese modo, en lugar de tener que procesar todos los nombres a la vez, lo hace de forma secuencial, línea a línea (y página a página).

El `prompt` también lleva en cuenta la tarea a llevar a cabo, la extracción de datos, la estructura de la información, las posibilidades de identificación de los diferentes tipos de ausencia. 

```{r extraccion, eval=eval}

# Crea un conjunto de ejemplos
# para extraer los datos
eje <- tribble(
  ~text, ~answer,
  "APP ACUÑA NUÑEZ, RICHARD aus PPK FLORES VÍLCHEZ, CLEMENTE PRE FA QUINTANILLA CHACÓN, ALBERTO PRE","APP;ACUÑA NUÑEZ, RICHARD;AUS\nPPK;FLORES VÍLCHEZ, CLEMENTE;PRE\nFA;QUINTANILLA CHACÓN, ALBERTO;PRE",
  "GPFP AGUILAR MONTENEGRO, WILMER PRE FA FORONDA FARRO, MARÍA ELENA aus GPFP RAMÍREZ GAMARRA, OSÍAS LO",
  "GPFP;AGUILAR MONTENEGRO, WILMER;PRE\nFA;FORONDA FARRO, MARÍA ELENA;aus\nGPFP;RAMÍREZ GAMARRA, OSÍAS ;LO",
  "GPFP FIGUEROA MINAYA, MODESTO PRE GPFP PONCE VILLARREAL DE V., YESENIA aus",
  "GPFP;FIGUEROA MINAYA, MODESTO;PRE\nGPFP;PONCE VILLARREAL DE V., YESENIA;aus")

# Elije el modelo
modelo <- "gemma2:2b"

# Crea un conjunto de datos vacío
# para acumular los resultados
d <- data.frame()

# Para cada archivo de texto
# (por temas de tiempo, solo se
# procesarán los primeros 2)
for(i in 1:2){

  # Lee las líneas del texto
  ll <- read_lines(tx$text[i])
  
  # Encuentra la seccion del 
  # texto que contiene la información
  nn <- which(stri_detect_fixed(ll, "ASISTENCIA")==TRUE)
  
  # Extrae la sección del texto
  # deseada
  ll <- ll[(nn[1]+1):(nn[2]-1)]
  
  # Crea una consulta
  q <- make_query(
    template = "{text}\n{prompt}",
    text = ll,
    prompt = "Utilizando las instrucciones anteriores como referencia, retorna una tabla con tres variables -grupo, diputado, status- separada por ; con los datos del texto para devolver solo la tabla con los datos extraídos: Grupo;diputado;status. OUTPUT: grupo;diputado;status",
    system = "Eres un modelo experto en extracción de atributos de un texto. A continuación se le proporciona un texto con informaciones sobre personas que han asistido a los plenos de debates del congreso de diputados de Perú. Los nombres se encuentran divididos en tres columnas. La información de cada diputado aparece como GRUPO APELLIDOS, NOMBRES STATUS, donde Grupo es el grupo parlamentario a que pertenece, Apellidos son los apellidos de la persona en cuestión, Nombres contiene los nombres de la persona y STATUS corresponde si ha estado presente (PRE), ausente (aus) o licenciado (LO/LE), fallecido (F), en la Junta de Portavoces (JP). NUNCA introduzca comentarios más allá del formato de salida deseado.",
    examples = eje)
  
  # Realiza la consulta
  out <- query(q, 
               model = modelo, 
               screen = F, 
               output = "data.frame",
               verbose = F,
               model_params = list(
                 seed=123,
                 temperature=0))
  
  # Convierte la salida en un data.frame
  la <- paste(out$response, collapse="\n")
  la <- read_delim(la, delim=";", 
                   col_names = F,
                   show_col_types=F)
  
  # Añade el id del documento
  la$doc <- tx$doc_id[i]
  
  # Acumula los resultados
  d <- rbind(d, la)

}

# Muestra los resultados
reactable(d, 
          sortable = T, 
          resizable = T)

```

Como vemos, el modelo ha sido capaz de identificar 260 observaciones para tres variables, 130 para cada uno de los dos textos. Esa información ya está lista para ser analizada. Le he añadido también el nombre del archivo original, que me permite identificar a qué sesión, fecha y documento *.pdf* pertenece cada asistencia.

## Análisis de datos

También podemos pedir a los modelos de IA que realice análisis de datos. En el código a continuación, utilizamos una base de datos sobre los países del mundo contenida en el paquete `poliscidata` (con datos de Pollock and Edwards, *an R
Companion to Essentials of Political Analysis*) para pedir que el modelo interprete los datos y el resultado de correlaciones -entre democracia y fraccionamiento étnico, por ejemplo- y un teste t de Student de la media en el índice de democratización entre países de mayoría musulmana y no musulmana. Se tratan de relaciones complejos y que representan retos para cualquier analista, no solo una IA. Veamos los resultados:


```{r analisis, results='asis', eval=eval}

# Carga el paquete con los datos
library(poliscidata)

# Prepara los datos
w <- world
w <- w[, c("country","dem_level4", "dem_score14", "frac_eth", "muslim", "pop_65_older")]
names(w) <- c("Country", "Democracy_Level", "Democracy_Score", "Ethnic_Fractionalization", "Muslim_Majority", "Population_65_and_Older")

# Transforma en una tabla de texto
wt <- knitr::kable(w)
wt <- paste0(wt, collapse="\n")

# Genera las correlaciones
co <- round(cor(w[,c("Democracy_Score", "Ethnic_Fractionalization","Population_65_and_Older")], use = "pairwise.complete.obs", method = "pearson"),3)

# Transforma en una tabla de texto
co <- knitr::kable(co)
co <- paste0(co, collapse="\n")


# Ejecuta el test t de Student
tt <- t.test(w$Democracy_Score~w$Muslim_Majority, conf.level=0.99)
tt <- paste0(tt, collapse="\n")

# Crea el prompt
prompt <- paste0("Actúa como un analista de datos de las ciencias sociales. Tu tarea consiste en analizar dos tablas con datos. La primera tabla contiene información nivel de democracia, el índice de democracia, la diversidad étnica, si la mayoría de la población es musulmana y el porcentaje de la población con 65 años o más en diversos países. Los datos de la tabla son los siguientes: \n\n",wt, "\n\n y la tabla de correlaciones entre el índice de democracia, la fracionalización étnica y el porcentaje de personas con 65 años o más es la siguiente: \n\n", co, ".\n\nLos resultados del teste t de diferencia entre medias entre el índice de democracia y si el país posee una mayoría musulmana es el siguiente: \n\n", tt,"\n\n. Con base en esos datos: \n\n1) realiza un análisis descriptivo de las variables; 2) realiza un análisis entre nivel de democracia y diversidad étnica; 3) realiza un análisis entre el índice de democracia y la mayoría musulmana; y, finalmente, 4) realiza un análisis entre el mayoría musulmana y la diversidad étnica.\n\nUna vez realizo el análisis, escribe una síntesis de unas 2000 palabras sobre el tema.")

# Ejecuta el modelo
library(ellmer)

model <- "gemini-2.5-flash-preview-05-20"

chat <- chat_gemini(
  base_url = "https://generativelanguage.googleapis.com/v1beta/",
  api_args = list(generationConfig = 
                    list(temperature=1, 
                         seed=1234)),  
  model = model, 
  echo="none")


out <- chat$chat(prompt)

# Devuelve la respuesta
cat(out)


```





## Simulación

Otra aplicación de la IA generativa para la investigación consiste en realizar simulaciones. Podemos utilizar la información de entrenamiento para simular expertos y pedir que respondan a preguntas o cuestionarios. Esto resulta útil en dos aspectos centrales. Primero, como forma de sistematizar la información sobre un tema. Segundo, para evaluar la capacidad de los modelos de IA generativa para responder preguntas complejas de forma precisa y coherente.

No exemplo abaixo, utilizamos el modelo Gemini de Google para simular un experto en la República de Weimar. Los modelos LLM suelen ser particularmente buenos cuando se trata de información histórica, puesto que existe una larga literatura sobre el tema y, en algunos casos, parte de esa literatura está disponible en formato digital y en dominio público.


```{r simulacion, eval=eval}

# Carga el paquete
library(ellmer)

# Define el modelo
model <- "gemini-2.5-flash-preview-04-17"

# Crea un objeto chat para
# acceder al modelo
chat <- chat_gemini(
  base_url = "https://generativelanguage.googleapis.com/v1beta/",
  api_args = list(generationConfig = 
                    list(temperature=0.8, 
                         seed=1234)),  
  model = model,
  echo = "none",
  system_prompt="Actúa como un historiador político experto en el sistema partidario de la República de Weimar. Responde preguntas y proporciona información sobre los partidos políticos de la República de Weimar. Debe ser preciso y objetivo en sus respuestas, evitando opiniones personales o juicios de valor.")

# Define la lista de partidos
part <- c("Kommunistische Partei Deutschlands (KPD)",
  "Sozialdemokratische Partei Deutschlands (SPD)",
  "Deutsche Demokratische Partei (DDP)",
  "Deutsche Volkspartei (DVP)",
  "Deutsche Zentrumspartei (Zentrum)",
  "Konservative Volkspartei (KVP)",
  "Nationalsozialistische Deutsche Arbeiterpartei (NSDAP)",
  "Deutsche Demokratische Partei (DDP)",
  "Bayerische Volkspartei (BVP)")
  

# Crea el prompt para la evaluación del
# compromiso democrático de los partidos
prompt <- paste0("Su tarea consiste en evaluar el nivel de compromiso democrático de los partidos políticos que existieron durante dicho periodo, basándote en evidencia histórica sobre su comportamiento parlamentario, su ideología, sus alianzas políticas y su actitud frente a la Constitución de Weimar y nunca para el período posterior.\n\nUtiliza la siguiente escala para tu evaluación:\n\n0: Completamente antidemocrático (hostil al sistema republicano y parlamentario).\n\n10: Completamente democrático (adhesión plena y activa al sistema republicano y parlamentario).\n\nN.S.: No se puede determinar con certeza.\n\nResponde únicamente con un número del 0 al 10 o con 'N.S.', sin añadir justificaciones ni explicaciones.\n\nPartido a evaluar: ", part,".")
                 
# Ejecuta el modelo
ro <- chat$chat(prompt)
  
# Guarda los resultados
ev <- as.numeric(stri_split_fixed(ro, "\n", simplify = T))

# Crea otro prompt para evaluar
# la ideología de los partidos
prompt <- paste0("Su tarea consiste en evaluar la ideología predominante de los partidos políticos en términos de su ubicación en el espectro político de la época, considerando su programa, discursos, acciones políticas, bases sociales y declaraciones oficiales.\n\nUtiliza la siguiente escala para tu evaluación ideológica:\n\n0: Extrema izquierda\n\n10: Extrema derecha\n\nN.S.: No se puede determinar con certeza\n\nResponde únicamente con un número del 0 al 10 o con 'N.S.', sin añadir justificaciones ni explicaciones.\n\n Partido a evaluar: ", part,".")

# Ejecuta el modelo
ro <- chat$chat(prompt)

# Guarda los resultados
id <- as.numeric(stri_split_fixed(ro, "\n", simplify = T))

# Organiza todo en un
# data.frame
d1 <- data.frame(Partido=part, 
                 Democracia=ev, 
                 Ideologia=id)

# Visualiza los resultados
reactable(d1, 
          sortable=T, 
          resizable=T)

```





